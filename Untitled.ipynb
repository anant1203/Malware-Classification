{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from operator import add\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Foo\").master('local[*]').config(\"spark.executor.memory\", '6G').config(\"spark.driver.memory\", '6G').config(\"spark.driver.maxResultSize\", '7G').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "conf = pyspark.SparkConf().setAppName(\"App\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '3G')\n",
    "        .set('spark.driver.memory', '5G')\n",
    "        .set('spark.driver.maxResultSize', '5G'))\n",
    "#sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '192.168.1.215'),\n",
       " ('spark.app.name', 'Foo'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '6G'),\n",
       " ('spark.executor.memory', '6G'),\n",
       " ('spark.driver.maxResultSize', '7G'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.id', 'local-1549241474454'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '42523')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.wholeTextFiles(\"/home/anant/data_science_practicum/Malware Classification/dataset/data\")\n",
    "rddy=sc.textFile(\"/home/anant/data_science_practicum/Malware Classification/dataset/small_data/y_small_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=rdd.zipWithIndex()\n",
    "rddy=rddy.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=rddy.map(lambda line: Row(label=line[0], \n",
    "                              id=line[1],)).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-539eb7c65b67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =rdd.map(lambda line: Row(data=line[0][1], \n",
    "                              id=line[1],filename=line[0][0])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf=df1.alias('a').join(df2.alias('b'),col('b.id') == col('a.id')).drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|                data|            filename|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|00401000 CB CB CB...|file:/home/anant/...|    6|\n",
      "|00401000 33 DA 2B...|file:/home/anant/...|    1|\n",
      "|10001000 6A FF 68...|file:/home/anant/...|    3|\n",
      "|00401000 A4 AC 4A...|file:/home/anant/...|    7|\n",
      "|00401000 C7 01 24...|file:/home/anant/...|    1|\n",
      "|00401000 E8 0B 00...|file:/home/anant/...|    1|\n",
      "+--------------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultantdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf=resultantdf.withColumn('data', F.regexp_replace('data', '\\\\b\\\\w{3,}\\\\s', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"data\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf=regexTokenizer.transform(resultantdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf=resultantdf.drop('filename').drop('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram = NGram(n=1, inputCol=\"data\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngramDataFrame = ngram.transform(resultantdf)\n",
    "#ngramDataFrame.select(\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngramDataFrame.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "cv=countVectors.fit(resultantdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf1=cv.transform(resultantdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultantdf1=resultantdf1.withColumn('label',resultantdf1['label'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    6|[cb, cb, cb, cb, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[33, da, 2b, d8, ...|(256,[0,1,2,3,4,5...|\n",
      "|    3|[6a, ff, 68, a3, ...|(256,[0,1,2,3,4,5...|\n",
      "|    7|[a4, ac, 4a, 00, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[c7, 01, 24, 04, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[e8, 0b, 00, 00, ...|(256,[0,1,2,3,4,5...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultantdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#label_stringIdx = StringIndexer(inputCol = \"label\", outputCol = \"outlabel\")\n",
    "#pipeline = Pipeline(stages=[regexTokenizer,countVectors,])\n",
    "# Fit the pipeline to training documents.\n",
    "#pipelineFit = pipeline.fit(resultantdf)\n",
    "#dataset = pipelineFit.transform(resultantdf)\n",
    "#dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 5\n",
      "Test Dataset Count: 1\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = resultantdf1.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+-----+----------+\n",
      "|words|features|probability|label|prediction|\n",
      "+-----+--------+-----------+-----+----------+\n",
      "+-----+--------+-----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"words\",\"features\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    1|[e8, 0b, 00, 00, ...|(256,[0,1,2,3,4,5...|[-6.9116262707315...|[1.39300001396765...|       6.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    6|[cb, cb, cb, cb, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[33, da, 2b, d8, ...|(256,[0,1,2,3,4,5...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r=resultantdf1.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-c0d33905e895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
