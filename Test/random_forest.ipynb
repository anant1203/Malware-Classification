{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the training file =filename.txt\n",
      "enter the test file =filetest.txt\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    6|[c7, 45, fc, 01, ...|(256,[0,1,2,3,4,5...|[0.0,68.0,0.0,8.0...|[0.0,0.68,0.0,0.0...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n",
      "accuracy= 0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from operator import add\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class train_small_malware(object):\n",
    "    \n",
    "    '''\n",
    "    This class was used to format, preprocessing, feature extraction, training an testing of data.\n",
    "   \n",
    "    methods:\n",
    "    dataformation: Reading data from X_small_train file and using it to pull data from GCP bucket\n",
    "                    and the using dataframe/rdd to form the required dataframe\n",
    "    preprocesisng: Using regular expression to remove line number as required. Then using in built packages\n",
    "                    such as regexeTokenizer, CountVectorizer, ngrams , creating pipeline.\n",
    "    train_test_model: using Random forest to train the model.using testing data to check the result.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def dataformation(self,sc,filename):\n",
    "        #initializing the pathname\n",
    "        pathname=\"/home/anant/data_science_practicum/Malware Classification/dataset/small_data/\"\n",
    "        #\"https://console.cloud.google.com/storage/browser/uga-dsp/project1/files/\"\n",
    "        extension=\".bytes\"\n",
    "        \n",
    "        #reading X_small_train file data into small_rdd \n",
    "        small_rdd=self.sparkContext.textFile(pathname+filename)\n",
    "        \n",
    "        #creating the bytes_text_files to read the file in the data folder in GCP Buket\n",
    "        filename_rdd=small_rdd.map(lambda x: ((\"/home/anant/data_science_practicum/Malware Classification/dataset/data/\"+x+extension)))\n",
    "        bytes_text_files = filename_rdd.reduce(lambda x, y: x + \",\" + y);\n",
    "        \n",
    "        #reading data of all the file mentioned in the X_small_train.txt\n",
    "        rdd=self.sparkContext.wholeTextFiles(bytes_text_files)\n",
    "        \n",
    "        #reading label from Y_small_train.txt and storing in the rdd\n",
    "        rddy=self.sparkContext.textFile(\"/home/anant/data_science_practicum/Malware Classification/dataset/small_data/\"+\"y_small_train.txt\")\n",
    "        \n",
    "        # adding the index value to the both rdd and rddy\n",
    "        rdd=rdd.zipWithIndex()\n",
    "        rddy=rddy.zipWithIndex()\n",
    "        \n",
    "        #creating dataframe for both the rdd with column name\n",
    "        df2=rddy.map(lambda line: Row(label=line[0],id=line[1])).toDF() #(id,label)\n",
    "        df1 =rdd.map(lambda line: Row(data=line[0][1],id=line[1],filename=line[0][0])).toDF() #(id,filename,data)\n",
    "        \n",
    "        #creating resultant dataframe by joining above two dataframe (data,filename,label)\n",
    "        resultantdf=df1.alias('a').join(df2.alias('b'),col('b.id') == col('a.id')).drop('id')\n",
    "        \n",
    "        return resultantdf\n",
    "    \n",
    "    def preprocessing(self,resultantdf):\n",
    "        \n",
    "        #removal of linenumber from each file data using regural expression\n",
    "        resultantdf=resultantdf.withColumn('data', F.regexp_replace('data', '\\\\b\\\\w{3,}\\\\s', ''))\n",
    "        \n",
    "        #using inbuilt regexTokenizer  api to tokenize the data\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"data\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "        resultantdf=regexTokenizer.transform(resultantdf)\n",
    "        resultantdf=resultantdf.drop('filename').drop('data') #not required column dropped\n",
    "        \n",
    "        #using ngrams library to take out ngrams\n",
    "        #ngram = NGram(n=1, inputCol=\"data\", outputCol=\"ngrams\")\n",
    "        #ngramDataFrame = ngram.transform(resultantdf)\n",
    "        #ngramDataFrame.select(\"ngrams\")\n",
    "        \n",
    "        # bag of words count usinf count vectorizer\n",
    "        countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "        cv=countVectors.fit(resultantdf)\n",
    "        resultantdf1=cv.transform(resultantdf)\n",
    "        resultantdf1=resultantdf1.withColumn('label',resultantdf1['label'].cast('int'))\n",
    "        \n",
    "        return(resultantdf1)\n",
    "      \n",
    "    def __init__(self, sc): \n",
    "        self.sparkContext = sc \n",
    "        \n",
    "        \n",
    "        \n",
    "class test_small_malware(object):\n",
    "    \n",
    "    '''\n",
    "    This class was used to format, preprocessing, feature extraction, training an testing of data.\n",
    "   \n",
    "    methods:\n",
    "    dataformation: Reading data from X_small_test file and using it to pull data from GCP bucket\n",
    "                    and the using dataframe/rdd to form the required dataframe\n",
    "    preprocesisng: Using regular expression to remove line number as required. Then using in built packages\n",
    "                    such as regexeTokenizer, CountVectorizer, ngrams , creating pipeline.\n",
    "    train_test_model: using Random forest to train the model.using testing data to check the result.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def dataformation(self,sc,filename):\n",
    "        #initializing the pathname\n",
    "        pathname=\"/home/anant/data_science_practicum/Malware Classification/dataset/small_data/\"\n",
    "        #\"https://console.cloud.google.com/storage/browser/uga-dsp/project1/files/\"\n",
    "        extension=\".bytes\"\n",
    "        \n",
    "        #reading X_small_train file data into small_rdd \n",
    "        small_rdd=self.sparkContext.textFile(pathname+filename)\n",
    "        \n",
    "        #creating the bytes_text_files to read the file in the data folder in GCP Buket\n",
    "        filename_rdd=small_rdd.map(lambda x: ((\"/home/anant/data_science_practicum/Malware Classification/dataset/data/\"+x+extension)))\n",
    "        bytes_text_files = filename_rdd.reduce(lambda x, y: x + \",\" + y);\n",
    "        \n",
    "        #reading data of all the file mentioned in the X_small_train.txt\n",
    "        rdd=self.sparkContext.wholeTextFiles(bytes_text_files)\n",
    "        \n",
    "        #reading label from Y_small_train.txt and storing in the rdd\n",
    "        rddy=self.sparkContext.textFile(\"/home/anant/data_science_practicum/Malware Classification/dataset/small_data/\"+\"y_small_test.txt\")\n",
    "        \n",
    "        # adding the index value to the both rdd and rddy\n",
    "        rdd=rdd.zipWithIndex()\n",
    "        rddy=rddy.zipWithIndex()\n",
    "        \n",
    "        #creating dataframe for both the rdd with column name\n",
    "        df2=rddy.map(lambda line: Row(label=line[0],id=line[1])).toDF() #(id,label)\n",
    "        df1 =rdd.map(lambda line: Row(data=line[0][1],id=line[1],filename=line[0][0])).toDF() #(id,filename,data)\n",
    "        \n",
    "        #creating resultant dataframe by joining above two dataframe (data,filename,label)\n",
    "        resultantdf=df1.alias('a').join(df2.alias('b'),col('b.id') == col('a.id')).drop('id')\n",
    "        \n",
    "        return resultantdf\n",
    "    \n",
    "    def preprocessing(self,resultantdf):\n",
    "        \n",
    "        #removal of linenumber from each file data using regural expression\n",
    "        resultantdf=resultantdf.withColumn('data', F.regexp_replace('data', '\\\\b\\\\w{3,}\\\\s', ''))\n",
    "        \n",
    "        #using inbuilt regexTokenizer  api to tokenize the data\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"data\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "        resultantdf=regexTokenizer.transform(resultantdf)\n",
    "        resultantdf=resultantdf.drop('filename').drop('data') #not required column dropped\n",
    "        \n",
    "        #using ngrams library to take out ngrams\n",
    "        #ngram = NGram(n=1, inputCol=\"data\", outputCol=\"ngrams\")\n",
    "        #ngramDataFrame = ngram.transform(resultantdf)\n",
    "        #ngramDataFrame.select(\"ngrams\")\n",
    "        \n",
    "        # bag of words count usinf count vectorizer\n",
    "        countVectors = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "        cv=countVectors.fit(resultantdf)\n",
    "        resultantdf1=cv.transform(resultantdf)\n",
    "        resultantdf1=resultantdf1.withColumn('label',resultantdf1['label'].cast('int'))\n",
    "        \n",
    "        return(resultantdf1)\n",
    "    \n",
    "    \n",
    "    def __init__(self, sc): \n",
    "            \n",
    "        self.sparkContext = sc \n",
    "        \n",
    "        \n",
    "    \n",
    "class model_malware(object):\n",
    "    \n",
    "    def train_test_Model(self,trainingData,testdata):\n",
    "        \n",
    "        # Using LogisticRegression to train the model\n",
    "        #lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "        #lrModel = lr.fit(trainingData)\n",
    "        \n",
    "        # Using RandomForestClassifier to train the model\n",
    "        rf = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\",numTrees = 100,maxDepth = 4,maxBins = 32)\n",
    "        # Train model with Training Data\n",
    "        rfModel = rf.fit(trainingData)\n",
    "        \n",
    "        # using the testdata to for prediction/accuracy\n",
    "        predictions = rfModel.transform(testdata)\n",
    "        predictions.show()\n",
    "        evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "        \n",
    "        return(evaluator.evaluate(predictions))\n",
    "    \n",
    "    def __init__(self, sc): \n",
    "        self.sparkContext = sc \n",
    "        \n",
    "    \n",
    "if __name__=='__main__' :\n",
    "        \n",
    "    spark = SparkSession .builder .appName(\"Malware_Classification\") .getOrCreate()   \n",
    "        \n",
    "    sc=spark.sparkContext\n",
    "    \n",
    "    #creating training object     \n",
    "    train=train_small_malware(sc)\n",
    "    \n",
    "    #creating testing object\n",
    "    test=test_small_malware(sc)\n",
    "    \n",
    "    #creating model object\n",
    "    model=model_malware(sc)\n",
    "    \n",
    "    #processing training data\n",
    "    train_File=input(\"Enter the training file =\")\n",
    "    df_train=train.dataformation(sc,train_File)\n",
    "    df_preprocessed_train=train.preprocessing(df_train)\n",
    "    \n",
    "    #processing testing data\n",
    "    test_File=input(\"enter the test file =\")\n",
    "    df_test=test.dataformation(sc,test_File)\n",
    "    df_preprocessed_test=test.preprocessing(df_test)\n",
    "    \n",
    "    #calculating accuracy\n",
    "    accuracy=model.train_test_Model(df_preprocessed_train,df_preprocessed_test)\n",
    "    print(\"accuracy=\",accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    6|[e8, 0b, 00, 00, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[33, da, 2b, d8, ...|(256,[0,1,2,3,4,5...|\n",
      "|    3|[c7, 01, 24, 04, ...|(256,[0,1,2,3,4,5...|\n",
      "|    7|[6a, ff, 68, a3, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[cb, cb, cb, cb, ...|(256,[0,1,2,3,4,5...|\n",
      "|    1|[a4, ac, 4a, 00, ...|(256,[0,1,2,3,4,5...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    6|[33, c0, c2, 10, ...|(256,[0,1,2,3,4,5...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicitions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-437ae4ceb93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predicitions' is not defined"
     ]
    }
   ],
   "source": [
    "predicitions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
